# 使用服务网格提高可观测性和管理

在本章中，将向您介绍服务网格的概念，并了解如何使用其功能处理微服务系统环境中的挑战，包括安全性、策略实施、恢复能力和流量管理。服务网格还可用于提供可观察性，即，可视化服务网格中微服务之间流量流动的能力

服务网格部分与我们在本书前面了解的 SpringCloud 和 Kubernetes 的功能重叠。但服务网格中的大部分功能是对 SpringCloud 和 Kubernetes 的补充，我们将在本章中看到。

本章将介绍以下主题：

*   介绍服务网格概念和 Istio，这是一种流行的开源实现
*   您还将学习如何执行以下操作：
    *   在库伯内特斯部署 Istio
    *   创建服务网格
    *   观察维修网
    *   保护服务网格
    *   确保服务网格具有弹性
    *   使用服务网格执行零停机部署
    *   使用 Docker Compose 测试微服务环境，以确保微服务中的源代码未锁定到 Kubernetes 或 Istio 中

# 技术要求

本书中描述的所有命令都是使用 macOS Mojave 在 MacBook Pro 上运行的，但是修改这些命令应该足够简单，可以在其他平台（如 Linux 或 Windows）上运行它们。

本章所需的唯一新工具是 Istio 的命令行工具`istioctl`。可通过以下命令使用自制软件进行安装：

```
brew install istioctl
```

本章的源代码可在 GitHub 的[上找到 https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18) 。

为了能够按照书中所述运行命令，您需要将源代码下载到一个文件夹中，并设置一个指向该文件夹的环境变量`$BOOK_HOME`。示例命令包括以下命令：

```
export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud
git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME
cd $BOOK_HOME/Chapter18
```

Java 源代码是为 Java8 编写的，并在 Java12 上进行了测试。本章使用 Spring Cloud 2.1、SR2（也称为**格林威治**版本）、Spring Boot 2.1.6 和 Spring 5.1.8，即编写本章时 Spring 组件的最新可用版本。源代码已经使用 Kubernetes V1.15 进行了测试。

本章中的所有源代码示例均来自`$BOOK_HOME/Chapter18`中的源代码，但在某些情况下，经过编辑以删除源代码中不相关的部分，如注释、导入和日志语句。

如果您想查看[第 18 章](18.html)、*中应用于源代码的更改，使用服务网格来提高可观察性和管理*，即使用 Istio 创建服务网格所需的更改，您可以将其与[第 17 章](17.html)、*的源代码进行比较实现 Kubernetes 功能作为替代*。您可以使用您喜爱的 diff 工具，比较两个文件夹`$BOOK_HOME/Chapter17`和`$BOOK_HOME/Chapter18`。

# 使用 Istio 的服务网格简介

服务网格是一个基础结构层，用于控制和观察服务（例如微服务）之间的通信。服务网格中的功能，例如，可观察性、安全性、策略实施、恢复力和流量管理*是通过控制和监控服务网格内的所有内部通信（即服务网格中的微服务之间）来实现的。服务网格中的一个核心组件是一个轻量级的**代理**组件，它被注入到将成为服务网格一部分的所有微服务中。所有进出微服务的流量都配置为通过其代理组件。代理组件在运行时由服务网格中的**控制平面**使用代理公开的 API 进行配置。控制平面还通过这些 API 从代理收集遥测数据，以可视化服务网格中的流量*

 *服务网格还包含一个**数据平面**，该数据平面由服务网格中所有微服务中的代理组件以及用于处理进出服务网格的外部传入和传出流量的单独组件组成。如下图所示：

![](img/6f4234c5-40c7-405d-856e-9e11009c242d.png)

服务网格的第一个公开实现是开源项目 Linkerd，由浮力（[管理 https://linkerd.io](https://linkerd.io) ），起源于 Twitter 的欺骗项目（[http://twitter.github.io/finagle](http://twitter.github.io/finagle) ）。它于 2016 年推出，一年后的 2017 年，IBM、谷歌和 Lyft 推出了开源项目 Istio（[https://istio.io](https://istio.io) ）。

Istio 中的核心组件之一，即代理组件，基于 Lyft 的特使代理（[https://www.envoyproxy.io](https://www.envoyproxy.io) ）。在撰写本章时，Linkerd 和 Istio 是两种最流行和最广泛使用的服务网格实现。在本章中，我们将使用 Istio

Istio 可以部署在各种环境中，包括 Kubernetes（参见[https://istio.io/docs/setup](https://istio.io/docs/setup) 。在 Kubernetes 上部署 Istio 时，其运行时组件被部署到单独的 Kubernetes 命名空间`istio-system`中。Istio 还附带了一套 Kubernetes**自定义资源定义**（**CRD**）。在 Kubernetes 中使用 CRD 来扩展其 API，即向其 API 添加新对象。添加的 Istio 对象用于配置如何使用 Istio。最后，Istio 附带了一个 CLI 工具`istioctl`，用于将 Istio 代理注入到参与服务网格的微服务中

如前所述，Istio 分为控制平面和数据平面。作为操作员，我们将通过在 Kubernetes API 服务器中创建 Istio 对象来定义所需的状态，例如，声明路由规则。控制平面将读取这些对象，并向数据平面中的代理发送命令，以根据所需状态（例如，配置路由规则）采取操作。代理处理微服务之间的实际通信，并向控制平面报告遥测数据。控制平面中的各个组件使用遥测数据来可视化服务网格中的情况。

在以下小节中，我们将介绍以下主题：

*   如何将 Istio 代理注入微服务
*   我们将在本章中使用的 Istio API 对象
*   Istio 中构成控制平面和数据平面的运行时组件
*   Istio 引入后微服务领域的变化

# 将 Istio 代理注入现有微服务

在前面几章中，我们在 Kubernetes 中部署的微服务作为 Kubernetes 吊舱中的单个容器运行（请参阅[第 15 章](15.html)中的*介绍 Kubernetes API 对象*部分、*介绍 Kubernetes*进行概述）。为了使微服务加入基于 Istio 的服务网格，将 Istio 代理注入到每个微服务中。这是通过向运行 Istio 代理的 pod 添加额外的容器来实现的。

为了支持主容器而添加到 pod 的容器，例如 Istio 代理，被称为*sideca**r*。

下图显示了如何将 Istio 代理作为侧车注入示例吊舱**吊舱 a**：

![](img/a824d18b-8f01-4c08-97ef-d697d7154fc8.png)

pod 中的主容器**容器 A**配置为通过 Istio 代理路由其所有流量。

Istio 代理可以在创建部署对象时自动注入，也可以使用`istioctl`工具手动注入。

在本章中，我们将手动注入 Istio 代理。原因是 Istio 代理不支持 MySQL、MongoDB 和 RabbitMQ 使用的协议，因此我们只将 Istio 代理注入使用 HTTP 协议的 POD 中。可以通过以下命令将 Istio 代理注入现有部署对象的 POD 中：

```
kubectl get deployment sample-deployment -o yaml | istioctl kube-inject -f - | kubectl apply -f -
```

乍一看，这个命令可能有些令人生畏，但实际上它只是三个独立的命令。前一个命令使用管道（即`|`字符）将其输出发送到下一个命令。让我们看一下每个命令：

1.  `kubectl get deployment`命令从 Kubernetes API 服务器获取名为`sample-deployment`的部署的当前定义，并以 YAML 格式返回其定义。
2.  `istioctl kube-inject`命令从`kubectl get deployment`命令读取定义，并在部署处理的 POD 中为 Istio 代理添加额外的容器。部署对象中现有容器的配置将更新，以便传入和传出流量通过 Istio 代理。
    命令返回部署对象的新定义，包括 Istio 代理的容器。
3.  `kubectl apply`命令从`istioctl kube-inject`命令读取更新的配置，并应用更新的配置。属于部署的 POD 的滚动升级将以我们之前看到的相同方式启动（请参阅[第 16 章](16.html)中的*执行滚动升级*部分，*将我们的微服务部署到 Kubernetes】*。

`kubernetes/scripts`文件夹中的部署脚本已扩展为使用`istioctl`注入 Istio 代理。详情请参见即将发布的*创建服务网格*一节。

# 介绍 Istio API 对象

Istio 使用其 CRD 扩展了 Kubernetes API 和许多对象。请参阅[第 15 章](15.html)、*Kubernetes 简介*中的*介绍 Kubernetes API 对象*部分，了解 Kubernetes API 的概述。在本章中，我们将使用以下 Istio 对象：

*   `Gateway`用于配置如何处理服务网格的传入流量和传出流量。网关依赖于将传入流量路由到 Kubernetes 服务的虚拟服务。我们将使用网关对象接受 DNS 名称`minikube.me`的传入流量，使用 HTTPS。有关详细信息，请参阅*Kubernetes 入口资源被 Istio 入口网关替换为边缘服务器*部分。
*   `VirtualService`用于定义服务网格中的路由规则。我们将使用虚拟服务来描述如何将传入流量从 Istio 网关路由到 Kubernetes 服务以及服务之间的路由。我们还将使用虚拟服务注入故障和延迟，以测试服务网格的可靠性和恢复能力。
*   `DestinationRule`用于定义路由（使用虚拟服务）到特定服务（即目的地）的流量的策略和规则。我们将使用目标规则来设置加密策略，以加密内部 HTTP 流量，并定义描述服务可用版本的服务子集。在执行从现有版本的 microservice 到新版本的零停机（蓝色/绿色）部署时，我们将使用服务子集。
*   `Policy`用于定义如何对请求进行身份验证。我们将使用策略要求对服务网格的传入请求使用基于 JWT 的 OAuth 2.0/OIDC 访问令牌进行身份验证。请参阅本章的*使用 OAuth 2.0/OIDC 访问令牌验证外部请求*部分。策略还可用于定义如何保护服务网格中的部分内部通信。例如，策略可以要求使用 HTTPS 对内部请求进行加密，或者允许纯文本请求。最后，`MeshPolicy`对象可用于定义应用于整个服务网格的全局策略。

# 在 Istio 中引入运行时组件

Istio 包含许多运行时组件，在使用哪些组件方面高度可配置，并对每个组件的配置提供细粒度控制。请参阅本章的*在 Kubernetes 集群*中部署 Istio 章节，了解我们将在本章中使用的配置信息。

在本章中使用的配置中，Istio 控制平面由以下运行时组件组成：

*   **飞行员**：**负责向所有侧车提供服务网格配置的更新。**
***   **混合器**：由两个不同的运行时组件组成：
    *   **策略**-强制执行网络策略，如身份验证、授权、速率限制和配额。
    *   例如，**遥测*****-*******采集遥测信息并发送给普罗米修斯****
    *****   **厨房**：**负责收集和验证配置信息，并分发给控制平面内的其他 Istio 组件。*****   **城堡**：负责内部使用证书的发放和轮换。*   **Kiali**：为服务网格提供可观察性，可视化网格中发生的事情。Kiali 是一个独立的开源项目（参见[https://www.kiali.io](https://www.kiali.io)*   **普罗米修斯**：对基于时间序列的数据执行数据摄取和存储，例如性能指标。
    普罗米修斯是一个独立的开源项目（参见[https://prometheus.io](https://prometheus.io) ）。*   **格拉法纳**：可视化在普罗米修斯收集的绩效指标和其他时间序列相关数据。Grafana 是一个独立的开源项目（参见[https://grafana.com](https://grafana.com) ）。*   **跟踪**：处理并可视化分布式跟踪信息。基于 Jaeger 的分布式跟踪开源项目（参见[https://www.jaegertracing.io](https://www.jaegertracing.io) ）。Jaeger 提供了与 Zipkin 相同类型的功能，我们在[第 14 章](14.html)、*理解分布式跟踪*中使用了 Zipkin。********

 ****Kiali 使用 web 浏览器访问，并集成 Grafana 以查看性能指标和 Jaeger 以可视化分布式跟踪信息。

Istio 数据平面由以下运行时组件组成：

*   **入口****网关**：处理到服务网格的传入流量
*   **E****格雷斯****网关**：处理来自服务网格的传出流量
*   所有带有 Istio 代理的吊舱都作为侧车注入

下图总结了 Istio 控制平面和数据平面中的运行时组件：

![](img/bc1d5d4f-c2d3-4504-8d01-506c8fec94c4.png)

在下一节中，我们将介绍因 Istio 的引入而应用于微服务领域的变化。

# 微服务领域的变化

正如我们在上一节中所看到的，Istio 提供的组件在功能上与当前微服务环境中使用的组件重叠：

*   Istio 入口网关可以充当边缘服务器，作为 Kubernetes 入口资源的替代方案。
*   与 Istio 捆绑在一起的 Jaeger 组件可以用于分布式跟踪，而不是 Zipkin。

在下面的两小节中，我们将了解 Kubernetes 入口资源为何和如何被 Istio 入口网关取代，以及 Zipkin 为何和如何被 Jaeger 取代。

# Kubernetes 入口资源替换为 Istio 入口网关作为边缘服务器

在上一章中，我们介绍了 Kubernetes 入口资源作为边缘服务器（参见[第 17 章](17.html)中的*替换 Spring 云网关*一节，*实现 Kubernetes 功能作为替代）*。不幸的是，入口资源无法配置为处理 Istio 附带的细粒度路由规则。相反，Istio 有自己的边缘服务器，即 Istio 入口网关，在前面的 Istio 一节中介绍了*引入运行时组件。Istio 入口网关通过创建前面在*介绍 Istio API 对象*一节中描述的`Gateway`和`VisualService`资源来使用。*

因此，以下 Kubernetes 入口资源的定义文件`kubernetes/services/base/ingress-edge-server.yml`和`kubernetes/services/base/ingress-edge-server-ngrok.yml`已被删除。Istio`Gateway`和`VirtualService`资源的定义文件将添加到*创建服务网格*部分。

Istio 入口网关使用不同于用于访问 Kubernetes 入口资源的 IP 地址的 IP 地址访问，因此我们还需要更新映射到主机名`minikube.me`的 IP 地址，我们在运行测试时使用该主机名。这在本章的*设置对 Istio 服务的访问*部分进行处理。

# 简化系统环境并用 Jaeger 替换 Zipkin

正如在 Istio 部分*介绍运行时组件时所提到的，Istio 内置了对使用 Jaeger*的分布式跟踪的支持。*使用 Jaeger，我们可以通过移除[第 14 章](14.html)、*理解分布式跟踪*中介绍的 Zipkin 服务器来卸载和简化微服务环境。*

以下更改已应用于源代码以删除 Zipkin 服务器：

*   所有微服务构建文件中对`org.springframework.cloud:spring-cloud-starter-zipkin`的依赖关系`build.gradle`已被删除。
*   三个 Docker Compose 文件`docker-compose.yml`、`docker-compose-partitions.yml`和`docker-compose-kafka.yml`中 Zipkin 服务器的定义已被删除。
*   已删除 Zipkin 的以下 Kubernetes 定义文件：
    *   `kubernetes/services/base/zipkin-server.yml`
    *   `kubernetes/services/overlays/prod/zipkin-server-prod.yml`

Jaeger 将安装在*创建服务网格*部分。

由于 Istio 的引入，微服务领域发生了变化。我们现在准备在 Kubernetes 集群中部署 Istio。

# 在 Kubernetes 群集中部署 Istio

在本节中，我们将学习如何在 Kubernetes 集群中部署 Istio，以及如何在其中访问 Istio 服务。

我们将使用 Istio 的 v1.2.4，这是本章编写时提供的最新版本。

我们将使用 Istio 的演示配置，该配置适合在开发环境中测试 Istio，也就是说，启用了大多数功能，但配置为最低限度地使用资源。

此配置不适用于生产使用和性能测试。

其他安装选项见[https://istio.io/docs/setup/kubernetes/install](https://istio.io/docs/setup/kubernetes/install) 。

要部署 Istio，请执行以下步骤：

1.  下载 Istio 如下：

```
cd $BOOK_HOME/Chapter18
curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.4 sh -
```

2.  使用以下命令确保 Minikube 实例已启动并正在运行：

```
minikube status
```

如果响应已启动并正在运行，则预期响应如下所示：

![](img/b73dc4d8-a721-40eb-87be-a3395fc8c84f.png)

3.  在 Kubernetes 中安装特定于 Istio 的自定义资源定义（CRD）：

```
for i in istio-1.2.4/install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
```

4.  在 Kubernetes 中安装 Istio 演示配置，如下所示：

```
kubectl apply -f istio-1.2.4/install/kubernetes/istio-demo.yaml
```

5.  等待 Istio 部署可用：

```
kubectl -n istio-system wait --timeout=600s --for=condition=available deployment --all
```

该命令将逐个报告 Istio 中可用的部署资源。在命令结束之前，预期有 12 条消息，如`deployment.extensions/NNN condition met`。这可能需要几分钟（或更长时间），具体取决于您的硬件和 internet 连接。

6.  使用以下命令将 Kiali 的配置映射更新为 Jaeger 和 Grafana 的 URL：

```
kubectl -n istio-system apply -f kubernetes/istio/setup/kiali-configmap.yml && \
kubectl -n istio-system delete pod -l app=kiali && \
kubectl -n istio-system wait --timeout=60s --for=condition=ready pod -l app=kiali
```

配置映射`kubernetes/istio/setup/kiali-configmap.yml`包含指向 Jaeger 和 Grafana 的 URL，这些 URL 使用下一节中使用的`minikube tunnel`命令设置的 DNS 名称。

Istio 现在部署在 Kubernetes 中，但在我们继续创建服务网格之前，我们需要了解一点如何在 Minikube 环境中访问 Istio 服务。

# 设置对 Istio 服务的访问

在上一节中用于安装 Istio 的演示配置中，我们需要解决一些与连接相关的问题；即其类型为`LoadBalancer`。

也可以在 Minikube 实例的 IP 地址上使用其节点端口，端口范围为`30000`-`32767`。不幸的是，Istio 中基于 HTTPS 的路由不能包括端口号；也就是说，必须通过 HTTPS 的默认端口（`443`访问 Istio 的入口网关。因此，不能使用节点端口。相反，必须使用负载平衡器才能将 Istio 的路由规则与 HTTPS 一起使用。

Minikube 包含一个可用于模拟本地负载平衡器的命令`minikube tunnel`。此命令为每个负载平衡的 Kubernetes 服务（包括 Istio 入口网关）分配一个外部 IP 地址。这为您提供了更新主机名`minikube.me`转换所需的内容，我们在测试中使用了该转换。主机名`minikube.me`现在需要转换为 Istio 入口网关的外部 IP 地址，而不是我们在前面章节中使用的 Minikube 实例的 IP 地址。

`minikube tunnel`命令还可以使用集群本地 Kubernetes 服务的 DNS 名称访问这些服务。DNS 名称基于命名约定：`{service-name}.{namespace}.svc.cluster.local`。例如，当隧道启动并运行时，可以使用 DNS 名称`kiali.istio-system.svc.cluster.local`从本地 web 浏览器访问 Istio 的 Kiali 服务

下图总结了如何访问 Istio 服务：

![](img/6df8153e-3139-4cc7-9cb2-0f832f9e5129.png)

执行以下步骤以设置 Minikube 隧道：

1.  使 Kubernetes 服务在本地可用。在单独的终端窗口中运行以下命令（隧道启动并运行时，该命令锁定终端窗口）：

```
minikube tunnel
```

请注意，此命令要求您的用户具有`sudo`权限，并且您在启动和关闭期间输入密码。命令要求输入密码需要几秒钟的时间，因此很容易出错！

2.  配置`minikube.me`解析为 Istio 入口网关 IP 地址如下：
    1.  获取 Istio 入口网关的`minikube tunnel`命令公开的 IP 地址，并将其保存在名为`INGRESS_HOST`的环境变量中：

```
INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
```

```
echo "$INGRESS_HOST minikube.me" | sudo tee -a /etc/hosts
```

![](img/466a71aa-3959-4801-8017-ce9c50266daf.png)

3.  使用以下命令验证是否可以通过隧道到达 Kiali、Jaeger 和 Grafana:

```
curl -o /dev/null -s -L -w "%{http_code}" http://kiali.istio-system.svc.cluster.local:20001/kiali/
curl -o /dev/null -s -L -w "%{http_code}" http://grafana.istio-system.svc.cluster.local:3000
curl -o /dev/null -s -L -w "%{http_code}" http://jaeger-query.istio-system.svc.cluster.local:16686
```

每个命令都应该返回`200`（OK）。

例如，如果您的计算机或虚拟机中的 Minikube 实例暂停或重新启动，`minikube tunnel`命令可以停止运行。在这些情况下，需要手动重新启动该命令。因此，如果您未能在`https://minikube.me`URL 上调用 API，或者如果 Kiali 的 web UI 无法到达 Jaeger 以可视化分布式跟踪，或者无法到达 Grafana 以可视化性能指标，请始终检查 Minikube 隧道是否正在运行，并在需要时重新启动它。

# 使用 minikube 隧道命令的额外好处

运行`minikube tunnel`命令还可以访问可能感兴趣的其他集群内部 Kubernetes 服务。一旦环境按照*运行创建服务网格的命令*一节中的描述启动并运行，就可以实现以下功能：

*   可以使用以下命令检查`product-composite`微服务的`health`端点：

```
curl -k http://product-composite.hands-on.svc.cluster.local:4004/actuator/health
```

关于端口`4004`的使用说明，请参见*观察维修网*一节。

*   可以使用以下命令访问 review 数据库中的 MySQL 表：

```
mysql -umysql-user-dev -pmysql-pwd-dev review-db -e "select * from reviews" -h mysql.hands-on.svc.cluster.local
```

*   可以通过以下命令访问`product`和`recommendations`数据库中的 MongoDB 集合：

```
mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev -p mongodb-pwd-dev --authenticationDatabase admin product-db --eval "db.products.find()"

mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev -p mongodb-pwd-dev --authenticationDatabase admin recommendation-db --eval "db.recommendations.find()"
```

*   可以使用以下 URL 访问 RabbitMQ 的 web UI:`http://rabbitmq.hands-on.svc.cluster.local:15672`。使用凭证`rabbit-user-dev`和`rabbit-pwd-dev`登录。

Minikube 隧道就位后，我们现在准备创建服务网格。

# 创建服务网格

部署了 Istio 之后，我们就可以创建服务网格了。我们将使用`kubernetes/scripts/deploy-dev-env.bash`脚本来设置开发和测试环境。

创建服务网格所需的步骤基本上与我们在[第 17 章](17.html)中使用的步骤相同，*将 Kubernetes 功能作为替代*实现（请参阅*使用 Kubernetes ConfigMaps、secrets 和 ingress 进行测试*一节）。在运行创建服务网格的命令之前，让我们先看看对 Kubernetes 定义文件添加了哪些内容来设置服务网格。

# 源代码更改

为了能够在 Istio 管理的服务网格中运行微服务，对 Kubernetes 定义文件应用了以下更改：

*   已更新部署脚本以插入 Istio 代理
*   Kubernetes 定义文件的文件结构已更改
*   已添加 Istio 的 Kubernetes 定义文件

让我们一个接一个地看一遍。

# 更新部署脚本以注入 Istio 代理

在`kubernetes/scripts`文件夹中的 Kubernetes、`deploy-dev-env.bash`和`deploy-prod-env.bash`中用于部署微服务的脚本已经更新，以将 Istio 代理注入五个微服务中，即`auth-server`、`product-composite`、`product`、`recommendation`和`review`服务。

`deploy-prod-env.bash`脚本将用于*执行零停机部署*部分*中*

 *先前在现有微服务中的*部分中描述的`istioctl kube-inject`命令已添加到两个部署脚本中，如下所示：*

```
kubectl get deployment auth-server product product-composite recommendation review -o yaml | istioctl kube-inject -f - | kubectl apply -f -
```

由于`kubectl apply`命令将启动滚动升级，因此添加了以下命令以等待升级完成：

```
waitForPods 5 'version=<version>'
```

在滚动升级过程中，我们将为每个微服务运行两个 pod：一个没有 Istio 代理的旧 pod 和一个注入了 Istio 代理的新 pod。`waitForPods`功能将等待旧 POD 终止；也就是说，滚动升级已经完成，只有五个新的吊舱正在运行。为了确定要等待的豆荚，使用了一个名为`version`的标签。在开发环境中，所有微服务吊舱都标有`version=latest`。

例如，产品微服务的部署文件`kubernetes/services/base/deployments/product-deployment.yml`具有以下`version`标签定义：

```
metadata:
  labels:
    version: latest
```

在*执行零停机部署*部分，我们将微服务从版本`v1`升级到`v2`，版本标签将设置为`v1`和`v2`。

最后，以下命令已添加到脚本中，以使它们等待部署及其 POD 准备就绪：

```
kubectl wait --timeout=120s --for=condition=Ready pod --all
```

在查看部署脚本中的更新之后，让我们看看由于引入了 Istio，Kubernetes 定义文件的文件结构受到了怎样的影响。

# 更改 Kubernetes 定义文件的文件结构

自[第 16 章](16.html)、*将我们的微服务部署到 Kubernetes*（参见*Kustomize*一节）以来，`kubernetes/services`中 Kubernetes 定义文件的文件结构有所扩展，现显示如下：

![](img/cc35c1d8-4834-4161-9759-347150bb845d.png)

`base`文件夹由三个子文件夹组成。原因是我们将在*执行零停机部署*部分同时运行两个版本的微服务，即每个微服务版本一个 pod。由于 pod 由部署对象管理，因此每个微服务也需要两个部署对象。为了实现这一点，部署对象的基本版本被放置在一个单独的文件夹`deployments`中。服务对象和 Istio 定义分别放在它们自己的基本文件夹中：`services`和`istio`。

在开发环境中，我们每个微服务只运行一个版本。它的 kustomization 文件`kubernetes/services/overlays/dev/kustomization.yml`已更新为包含所有三个文件夹作为基本文件夹：

```
bases:
- ../../base/deployments
- ../../base/services
- ../../base/istio
```

请参阅以下*执行零停机部署*部分，了解如何使用生产环境设置部署两个并发版本的微服务的详细信息。

现在，让我们来看看 Istio 文件夹中的新文件。

# 为 Istio 添加 Kubernetes 定义文件

Istio 定义已添加到`istio`文件夹中。本节中感兴趣的 Istio 文件是网关定义及其相应的虚拟服务。其他 Istio 文件将在*使用 OAuth 2.0/OIDC 访问令牌对外部请求进行身份验证*和*使用相互身份验证（mTLS）*保护内部通信部分进行说明。

Istio 网关在`kubernetes/services/base/istio/gateway.yml`文件中声明，如下所示：

```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: hands-on-gw
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - "minikube.me"
    port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
      privateKey: /etc/istio/ingressgateway-certs/tls.key
```

以下是对上述源代码的一些解释：

*   网关名为`hands-on-gw`；下面的虚拟服务使用此名称。
*   `selector`字段指定网关资源将由内置的 Istio 入口网关处理。
*   `hosts`和`port`字段指定网关将通过端口`443`使用 HTTPS 处理`minikube.me`主机名的传入请求。
*   `tls`字段指定 Istio 入口网关可以在何处找到用于 HTTPS 通信的证书和私钥。有关如何创建这些证书文件的详细信息，请参阅*使用 HTTPS 和证书保护外部端点*部分。

将请求从网关路由到`product-composite`服务的虚拟服务对象`kubernetes/services/base/istio/product-composite-virtual-service.yml`如下所示：

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: product-composite-vs
spec:
  hosts:
  - "minikube.me"
  gateways:
  - hands-on-gw
  http:
  - match:
    - uri:
        prefix: /product-composite
    route:
    - destination:
        port:
          number: 80
        host: product-composite
```

对上述源代码的解释如下：

*   `hosts`字段指定虚拟服务将路由发送到主机`minikube.me`的请求
*   `match`和`route`块指定包含以`/product-composite`开头的 URI 的请求将转发到名为`product-composite`的 Kubernetes 服务。

在前面的源代码中，目标主机是使用其短名称指定的，换句话说，`product-composite`。这是因为本章中的示例将所有 Kubernetes 定义保存在一个相同的名称空间`hands-on`中。如果不是这样，建议在 Istio 文档中使用主机的**完全限定域名**（**FQDN**），即`product-composite.hands-on.svc.cluster.local`。

最后，用于将请求从网关路由到身份验证服务器的虚拟服务对象`kubernetes/services/base/istio/auth-server-virtual-service.yml`看起来非常相似，区别在于它将以`/oauth`开头的请求路由到 Kubernetes 服务`auth-server`。

源代码中的这些更改就绪后，我们现在就可以创建服务网格了。

# 运行命令以创建服务网格

通过运行以下命令创建服务网格：

1.  使用以下命令从源代码生成 Docker 映像：

```
cd $BOOK_HOME/Chapter18
eval $(minikube docker-env)
./gradlew build && docker-compose build
```

2.  重新创建`hands-on`名称空间，并将其设置为默认名称空间：

```
kubectl delete namespace hands-on
kubectl create namespace hands-on
kubectl config set-context $(kubectl config current-context) --namespace=hands-on 
```

3.  通过使用以下命令运行`deploy-dev-env.bash`脚本来执行部署：

```
./kubernetes/scripts/deploy-dev-env.bash 
```

4.  部署完成后，请验证每个微服务吊舱中是否有两个容器：

```
kubectl get pods
```

期望得到以下方面的响应：

![](img/f0040d84-3544-45f4-9dd2-fd7a30e5ea31.png)

请注意，运行我们的微服务的 pod 报告每个 pod 有两个容器；也就是说，他们将 Istio 代理作为副驾注入！

5.  使用以下命令运行常规测试：

```
./test-em-all.bash
```

`script test-em-all.bash`的默认值已从前面的章节中更新，以适应在 Minikube 中运行的 Kubernetes。

预期输出与我们在前面章节中看到的类似：

![](img/f18d0079-84bb-4a36-96db-00e71ec593df.png)

6.  您可以通过运行以下命令手动试用 API：

```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)

curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .productId
```

在响应中需要请求的产品 ID`2`。

随着服务网格的启动和运行，让我们看看如何使用 Kiali 观察服务网格中发生的事情！

# 观察服务网

在本节中，我们将使用 Kiali 和 Jaeger 来观察服务网格中的情况。使用 Grafana 进行性能监控，请参见[第 20 章](20.html)、*监控微服务*

在我们这样做之前，我们需要消除 Kubernetes 的活力和准备度探测器进行健康检查时产生的一些噪音。在前面的章节中，它们使用了与 API 请求相同的端口。这意味着 Istio 将为健康检查和发送到 API 的请求收集远程通信数据。这将导致 Kiali 显示的图形变得不必要的混乱。Kiali 可以过滤掉我们不感兴趣的流量，但更简单的解决方案是使用不同的端口进行健康检查。

微服务可以配置为使用单独的端口发送到执行器端点的请求，例如，发送到`/actuator/health`端点的健康检查。以下行已添加到所有微服务的通用配置文件`config-repo/application.yml`：

```
management.server.port: 4004
```

这将使所有微服务使用端口`4004`公开健康端点。`kubernetes/services/base/deployments`文件夹中的所有部署文件都已更新，以便在其活动性和就绪性探测中使用端口`4004`。

SpringCloudGateway（保留此端口以便我们可以在 Docker Compose 中运行测试）将继续使用相同的端口来请求 API 和`health`端点。在`config-repo/gateway.yml`配置文件中，管理端口恢复为 API 使用的端口：

```
management.server.port: 8443
```

通过将请求发送到健康端点，我们可以开始通过服务网格发送一些请求

我们将使用`siege`启动低容量负载测试，我们在[第 16 章](16.html)中了解到，*将我们的微服务部署到 Kubernetes*（请参阅*执行滚动升级*一节）。之后，我们将介绍 Kiali 的一些最重要的部分，以了解如何使用 Kiali 观察服务网格。我们还将探讨 Kiali 与 Jaeger 的集成以及 Jaeger 如何用于分布式跟踪：

使用以下命令启动测试：

```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)

siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1
```

第一个命令将获得一个 OAuth 2.0/OIDC 访问令牌，该令牌将在下一个命令中使用，`siege`用于每秒向产品复合 API 提交一个 HTTP 请求。

`siege`命令的预期输出如下：

![](img/124bc0e4-2fda-4b01-be2f-81c0653eaa0a.png)

1.  在 web 浏览器中使用`http://kiali.istio-system.svc.cluster.local:20001/kiali`URL 打开 Kiali 的 web UI，并使用以下用户名和密码登录：`admin`和`admin`。预期会出现类似以下内容的网页：

![](img/621bc3a8-d0ad-4254-9de3-fea188d12428.png)

2.  如果尚未激活，请单击“概述”选项卡。
3.  单击实践名称空间中的图形图标。预计将显示一个图，表示通过服务网格的当前交通流，沿以下线：

![](img/8e6dcb80-2f70-4714-acb9-b8abcf8e49cd.png)

4.  单击显示按钮，取消选择服务节点，然后选择交通动画。
    Kiali 显示一个图形，表示当前通过服务网格发送的请求，其中活动请求由带箭头的小移动圆圈表示。
    这对服务网格中正在发生的事情给出了一个非常好的初步概述！

5.  现在让我们看看使用 Jaeger 的一些分布式跟踪：

![](img/36f57f31-b217-474d-8bd6-99ddb55b7370.png)

6.  单击产品节点。

7.  点击服务：产品链接。在服务的网页上，单击菜单中的 Traces 选项卡，Kiali 将使用 Jaeger 显示产品服务所涉及的跟踪的嵌入式视图。预期会出现以下网页：

![](img/4e530bf8-fba4-4321-9f45-450dfb3fe9dc.png)

8.  单击其中一条记录道进行检查。预期会出现以下网页：

![](img/402e5bb3-c03c-48dc-a5aa-ed14e971ecb0.png)

这与 Zipkin 的追踪信息基本相同，在[第 14 章](14.html)*理解分布式追踪*中提供。

还有很多东西需要探索，但作为一个介绍，这就足够了。您可以自行探索 Kiali、Jaeger 和 Grafana 中的 web UI。

在[第 20 章](20.html)*监控微服务*中，我们将进一步探讨性能监控能力。

让我们继续学习如何使用 Istio 提高服务网格中的安全性！

# 保护服务网

在本节中，我们将学习如何使用 Istio 来提高服务网格的安全性。我们将讨论以下主题：

*   如何使用 HTTPS 和证书保护外部端点
*   如何要求使用 OAuth 2.0/OIDC 访问令牌对外部请求进行身份验证
*   如何使用相互身份验证（mTLS）保护内部通信

现在，让我们在以下各节中了解其中的每一项。

# 使用 HTTPS 和证书保护外部端点

在*创建服务网格*一节中，我们看到了如何将 Istio 入口网关配置为使用以下证书文件来保护使用 HTTPS 发送到`minikube.me`的外部请求。Istio 入口网关配置如下：

```
spec:
  servers:
  - hosts:
    - "minikube.me"
    ...
    tls:
      mode: SIMPLE
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
      privateKey: /etc/istio/ingressgateway-certs/tls.key 
```

但你可能会问，这些文件是从哪里来的？

通过运行以下命令，我们可以看到如何配置 Istio 入口网关：

```
kubectl -n istio-system get deploy istio-ingressgateway -o json
```

我们会发现它准备挂载一个名为`istio-ingressgateway-certs`的可选机密，并且它将被映射到文件夹`/etc/istio/ingressgateway-certs/`：

![](img/206f48a3-7bfa-42df-97ad-e1be388adda9.png)

这将导致在`/etc/istio/ingressgateway-certs/tls.crt`和`/etc/istio/ingressgateway-certs/tls.key`文件路径上向 Istio 入口网关提供名为`istio-ingressgateway-certs`的秘密的证书文件`tls.crt`和`tls.key`。

通过`kubernetes/scripts`文件夹中的`deploy-dev-env.bash`和`deploy-prod-env.bash`部署脚本，通过以下命令创建此秘密：

```
kubectl create -n istio-system secret tls istio-ingressgateway-certs \
--key kubernetes/cert/tls.key --cert kubernetes/cert/tls.crt
```

证书文件是在[第 17 章](17.html)*中创建的，作为替代*实现 Kubernetes 功能（请参阅*使用 Kubernetes ConfigMaps、secrets 和 ingress 进行测试*部分）。

为了验证 Istio 入口网关使用的是这些证书，我们可以运行以下命令：

```
keytool -printcert -sslserver minikube.me:443 | grep -E "Owner:|Issuer:"
```

预期以下输出：

![](img/a5d99e95-d5ab-49fc-b2c3-62afb06295d1.png)

输出显示证书是为`minikube.se`颁发的，并且是自签名的；即发行人也是`minikube.me`。

此自签名证书可以替换为可信证书颁发机构（CA）为生产用例购买的证书。Istio 最近增加了对使用证书管理器和 Let's Encrypt 自动设置受信任证书的支持，正如我们在[第 17 章](17.html)中所做的那样，*实现 Kubernetes 功能作为替代*（参考*使用证书管理器设置证书，让我们加密*部分）。这种支持目前有点过于复杂，不适合本章

在验证了证书配置之后，让我们继续了解 Istio 入口网关如何保护微服务免受未经验证的请求**。**

# 使用 OAuth 2.0/OIDC 访问令牌验证外部请求

Istio 入口网关能够要求并验证基于 JWT 的 OAuth 2.0/OIDC 访问令牌，换句话说，保护服务网格中的微服务免受外部未经验证的请求。有关 JWT、OAuth 2.0 和 OIDC 的概述，请参阅[第 11 章](17.html)、*安全访问 API*（请参阅*使用 OAuth 2.0 和 OpenID Connect 验证和授权 API 访问*部分）。

为了启用身份验证，我们需要创建一个 Istio`Policy`对象，该对象指定应该保护哪些目标以及应该信任哪些访问令牌颁发者，即 OAuth 2.0/OIDC 提供者。这在`kubernetes/services/base/istio/jwt-authentication-policy.yml`文件中完成，如下所示：

```
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "jwt-authentication-policy"
spec:
  targets:
  - name: product-composite
  peers:
  - mtls:
      mode: PERMISSIVE
  origins:
  - jwt:
      issuer: "http://auth-server.local"
      jwksUri: "http://auth-server.hands-on.svc.cluster.local/.well-known/jwks.json"
  principalBinding: USE_ORIGIN
```

对上述源代码的解释如下：

*   `targets`列表指定将对发送到`product-composite`微服务的请求执行身份验证检查。
*   `origins`列表指定了我们所依赖的 OAuth 2.0/OIDC 提供程序。对于每个颁发者，将指定颁发者的名称及其 JSON web 密钥集的 URL。有关概述，请参阅[第 11 章](11.html)、*确保访问 API*（请参阅*介绍 OpenId Connect*一节）。我们已指定本地身份验证服务器`http://auth-server.local`。

策略文件是由`kubernetes/scripts/deploy-dev-env.bash`部署脚本应用的，在*运行的命令中使用它来创建服务网格*部分。

验证 Istio 入口网关而非`product-composite`微服务拒绝无效请求的最简单方法是发出一个没有访问令牌的请求，并观察返回的错误消息。如果身份验证失败，Istio 入口网关返回以下错误消息`Origin authentication failed.`，而`product-composite`微服务返回空字符串。两者都返回 HTTP 代码`401`（未经授权）。

请使用以下命令进行尝试：

1.  在不使用访问令牌的情况下，按照以下方式发出请求：

```
curl https://minikube.me/product-composite/2 -kw " HTTP Code: %{http_code}\n"
```

期待回复说`Origin authentication failed. HTTP Code: 401`

2.  使用以下命令临时删除策略：

```
kubectl delete -f kubernetes/services/base/istio/jwt-authentication-policy.yml
```

请等待一分钟，以允许将该策略更改传播到 Istio 入口网关，然后在没有访问令牌的情况下重试该请求。响应现在应该只包含 HTTP 代码：`HTTP Code: 401`。

3.  使用以下命令再次启用策略：

```
kubectl apply -f kubernetes/services/base/istio/jwt-authentication-policy.yml
```

**Suggested additional exercise:** Try out the Auth0 OIDC provider, as described in [Chapter 11](11.html), *Securing Access to APIs* (refer to the T*esting with an OpenID Connect provider, Auth0,* section. Add your Auth0 provider to `jwt-authentication-policy.yml`. In my case, it appears as follows:

```
  - jwt:
      issuer: "https://dev-magnus.eu.auth0.com/" 
      jwksUri: "https://dev-magnus.eu.auth0.com/.well-known/jwks.json"
```

现在，让我们转到 Istio 中介绍的最后一种安全机制——使用相互身份验证 mTLS 自动保护服务网格中的内部通信。

# 使用相互身份验证（mTLS）保护内部通信

在本节中，我们将学习如何配置 Istio，以使用*相互**认证*、mTLS 自动保护服务网格内的内部通信。在使用相互身份验证时，不仅服务端通过公开证书来证明其身份，而且客户端还通过公开客户端证书向服务器证明其身份。与正常的 TLS/HTTPS 使用相比，这提供了更高级别的安全性，其中只有服务器的身份得到验证。建立和维护相互认证；也就是说，新证书的提供和过期证书的轮换是复杂的，因此很少使用。Istio 完全自动化用于服务网格内部通信的相互身份验证证书的设置和轮换。这使得使用相互身份验证比手动设置要容易得多。

那么，我们为什么要使用相互认证呢？用 HTTPS 和 OAuth 2.0/OIDC 访问令牌保护外部 API 还不够吗？

只要攻击通过外部 API 进行，就足够了。但是如果库伯内特斯星系团内的一个吊舱受到破坏怎么办？例如，如果攻击者获得对 pod 的控制权，则攻击者可以开始监听 Kubernetes 群集中其他 pod 之间的通信。如果内部通信以纯文本形式发送，攻击者将很容易访问群集中 POD 之间发送的敏感信息。为了最大限度地减少此类入侵造成的损害，可以使用相互身份验证来防止攻击者窃听内部网络流量。

要启用由 Istio 管理的相互身份验证，需要在服务器端使用策略配置 Istio，在客户端使用目标规则配置 Istio。

当使用 Istio 的演示配置时，正如我们在 Kubernetes 集群中*部署 Istio 部分中所做的那样，我们创建了一个全局网格策略，该策略将服务器端配置为使用许可模式，这意味着 Istio 代理将同时允许纯文本和加密请求。这可以通过以下命令进行验证：*

```
kubectl get meshpolicy default -o yaml
```

预期会出现类似以下情况的响应：

![](img/adef0bf8-61fd-484e-854c-a07e50609603.png)

要将微服务配置为在内部向其他微服务发送请求时使用相互身份验证，将为每个微服务创建一个目标规则。这在`kubernetes/services/base/istio/internal_mtls_destination_rules.yml`文件中完成。目标规则看起来都一样；例如，对于`product-composite`服务，它们显示如下：

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: product-composite-dr
spec:
  host: product-composite
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
```

`trafficPolicy`设置为使用`tls`和`ISTIO_MUTUAL`，表示相互认证由 Istio 管理。

当`kubernetes/scripts/deploy-dev-env.bash`部署脚本在前面的*运行命令中用于创建服务网格*部分时，目标规则由`kubernetes/scripts/deploy-dev-env.bash`部署脚本应用。

要验证内部通信是否受到保护，请执行以下步骤：

1.  确保在前面观察检修网部分的*中开始的负载测试仍在运行。*
2.  在 web 浏览器中转到 Kiali 图形（[http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali) ）。
3.  单击显示按钮以启用安全标签。该图将显示受 Istio 自动相互认证保护的所有通信链路上的挂锁，如下所示：

![](img/cc3e1088-449c-4140-8da3-9d90d416f1ab.png)

除了到资源管理器（RabbitMQ、MySQL 和 MongoDB）的链接之外，所有链接都需要挂锁。

对 RabbitMQ、MySQL 和 MongoDB 的调用不由 Istio 代理处理，因此需要使用 TLS 对手动配置进行保护。

有了这些，我们已经看到了 Istio 中的所有三种安全机制都在运行，现在是时候看看 Istio 如何帮助我们验证服务网格是否具有弹性了。

# 确保服务网格具有弹性

在本节中，我们将学习如何使用 Istio 来确保服务网格具有弹性；也就是说，它可以处理服务网格中的临时故障。Istio 在超时、重试和一种称为**异常检测**的断路器来处理临时故障方面提供了类似于 Spring 框架的机制。在决定是否应使用语言本机机制来处理临时故障，或者是否应将其委托给 Istio 等服务网格时，我倾向于使用语言本机机制，如[第 13 章](13.html)中的示例所示，*使用 Resilience4J*提高弹性。在许多情况下，保持处理错误的逻辑非常重要，例如，处理断路器的备用方案，以及微服务的其他业务逻辑

在某些情况下，Istio 中的相应机制可能会有很大帮助。例如，如果部署了一个微服务，并且确定它无法处理生产中不时出现的临时故障，那么使用 Istio 添加超时或重试机制，而不是等待新版本的微服务以及相应的错误处理功能，这是非常方便的。

Istio 在恢复能力方面的另一个功能是将故障和延迟注入现有服务网格的能力。为什么会有人想这么做？

以受控方式注入故障和延迟对于验证微服务中的弹性功能是否按预期工作非常有用！我们将在本节中试用它们，验证`product-composite`微服务中的重试、超时和断路器是否按预期工作。

在[第 13 章](13.html)中*使用 Resilience4j*提高弹性（参见*添加可编程延迟和随机错误*一节），我们在微服务源代码中添加了对注入故障和延迟的支持。最好使用 Istio 在运行时注入故障和延迟的功能来替换源代码，如下所示。

我们将首先注入故障，看看`product-composite`微服务中的重试机制是否如预期的那样工作。之后，我们将延迟产品服务的响应，并验证断路器是否按预期处理延迟

# 通过注入故障测试恢复力

让我们让产品服务抛出随机错误，并验证 microservice 环境是否正确处理了这一问题。我们期望`product-composite`微服务中的重试机制启动并重试请求，直到请求成功或达到其最大重试次数限制。这将确保短期故障对最终用户的影响不会超过重试尝试带来的延迟。请参阅[第 13 章](13.html)*中的*添加重试机制*部分*使用 Resilience4j*提高弹性，以了解`product-composite`微服务中重试机制的概述。*

可以使用`kubernetes/resilience-tests/product-virtual-service-with-faults.yml`注入故障。其内容如下：

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: product-vs
spec:
  hosts:
    - product
  http:
  - route:
    - destination:
        host: product
    fault:
 abort:
        httpStatus: 500
        percent: 20
```

该定义指出，发送到产品服务的请求中有 20%将以 HTTP 状态代码 500（内部服务器错误）中止。

执行以下步骤以测试此功能：

1.  确保使用`siege`进行的负载测试（如在*观察服务网**部分中开始的）正在运行。* 
**   使用以下命令应用故障注入：*

 *```
kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-faults.yml
```

3.  监控`siege`负载测试工具的输出。预期输出类似于以下内容：

![](img/0bc01f04-88dd-4507-8e09-21be25135194.png)

从示例输出中，我们可以看到所有请求仍然成功，换句话说，返回状态 200（OK）；然而，其中一些（20%）需要额外的一秒钟才能完成。这表示`product-composite`微服务中的重试机制已启动，并已重试对产品服务的失败请求。

4.  Kiali 还将指出发送到产品服务的请求有问题，如下所示：
    1.  转到 Kiali 的 web UI 中的调用图，我们之前使用它来观察名称空间中的流量，`hands-on`。
    2.  单击显示菜单按钮并选择服务节点。
    3.  单击“显示”按钮左侧名为“无边缘标签”的菜单按钮，然后选择“响应时间”选项。

![](img/ee4ac35f-e8a5-437f-b1de-261228e0ed60.png)

指向服务节点产品的箭头将显示为红色，表示检测到失败的请求。如果单击箭头，我们可以看到右侧的故障统计信息。

在前面的示例屏幕截图中，报告了 19.4%的错误率，这与我们要求的 20%相当。请注意，从 Istio 网关到`product-composite`服务的箭头仍为绿色。这意味着`product-composite`服务中的重试机制保护最终用户；换句话说，故障不会传播到最终用户。

使用以下命令结束故障注入的移除：

```
kubectl delete -f kubernetes/resilience-tests/product-virtual-service-with-faults.yml
```

现在让我们进入下一节，在这里我们将注入延迟来触发断路器。

# 通过注入延迟测试弹性

从[第 13 章](13.html)*使用弹性 4J*提高弹性（参考*介绍断路器*一节），我们知道断路器可用于防止由于服务响应缓慢或服务根本不响应而导致的问题。

让我们通过使用 Istio 向产品服务中注入延迟来验证`product-composite`服务中的断路器是否按预期工作。可以使用虚拟服务注入延迟

参见`kubernetes/resilience-tests/product-virtual-service-with-delay.yml`。其代码如下：

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: product-vs
spec:
  hosts:
    - product
  http:
  - route:
    - destination:
        host: product
    fault:
      delay:
        fixedDelay: 3s
        percent: 100
```

上述定义规定，发送到产品服务的所有请求应延迟 3 秒。

从`product-composite`服务发送到产品服务的请求配置为 2 秒后超时。断路器配置为在连续 3 次请求失败时断开其电路。当电路开路时，它将快速失效；换句话说，它将立即抛出异常，而不尝试调用底层服务。`product-composite`微服务中的业务逻辑将捕获此异常并应用回退逻辑。有关概述，请参见[第 13 章](13.html)、*使用弹性 4J*提高弹性（参见*添加断路器*一节）。

执行以下步骤，通过注入延迟来测试断路器：

1.  在`siege`正在运行的终端窗口中按*Ctrl+C*，通过`siege`命令停止负载试运行
2.  使用以下命令在产品服务中创建临时延迟：

```
kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml
```

3.  按如下方式获取访问令牌：

```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)
```

4.  连续发送六个请求。预计电路在前三次失败呼叫后断开，断路器对最后三次呼叫应用快速失败逻辑，并返回回退响应，如下所示：

```
for i in {1..6}; do time curl -k https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN"; done
```

前 3 个调用的响应应该是与超时相关的错误消息，响应时间为 2 秒，换句话说，是超时时间。预计前 3 次电话的响应如下所示：

**![](img/c61d561d-e023-488b-9e75-1dbb32bd2f5c.png)**

来自最后 3 个调用的响应预计是来自回退逻辑的响应，响应时间较短。最后 3 次电话的回复如下：

![](img/468fa43c-0a38-47fa-8baf-80de72b55711.png)

1.  通过使用以下命令删除临时延迟，模拟延迟问题已修复的事实：

```
kubectl delete -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml
```

2.  通过使用前面的命令发送新请求，验证是否再次返回正确的答案，并且没有任何延迟。

如果您想检查断路器的状态，可以通过以下命令进行：

**`curl product-composite.hands-on.svc.cluster.local:4004/actuator/health -s | jq -r .details.productCircuitBreaker.details.state`**

根据其状态报告`CLOSED`、`OPEN`或`HALF_OPEN`。

这证明了当我们使用 Istio 注入延迟时，断路器的反应与预期一致。这就结束了 Istio 中可用于验证微服务环境是否具有弹性的测试功能。我们将在 Istio 中探讨的最后一个功能是它对交通管理的支持；我们将确定如何使用它来实现零停机时间的部署。

# 执行零停机部署

如[第 16 章](16.html)所述，*将我们的微服务部署到 Kubernetes*（参见*执行滚动升级*一节），随着越来越多的自主微服务独立更新，能够在不停机的情况下部署更新变得至关重要。

在本节中，我们将了解 Istio 的流量管理和路由功能，以及如何使用这些功能在不需要停机的情况下执行新版本微服务的部署。在[第 16 章](16.html)中*将我们的微服务部署到 Kubernetes*（请参阅*执行滚动升级*一节），我们了解了如何使用 Kubernetes 执行滚动升级而不需要任何停机时间。使用 Kubernetes 滚动升级机制可以自动化整个过程，但不幸的是，在所有用户都被路由到新版本之前，没有提供测试新版本的选项。

使用 Istio，我们可以部署新版本，但最初会将所有用户路由到现有版本（在本章中称为旧版本）。之后，我们可以使用 Istio 的细粒度路由机制来控制如何将用户路由到新版本和旧版本。我们将了解如何使用 Istio 实现两种流行的升级策略：

*   **金丝雀部署*：***使用金丝雀部署时，除一组选定的测试用户路由到新版本外，大多数用户路由到旧版本。当测试用户批准了新版本后，可以使用蓝色/绿色部署将普通用户路由到新版本
*   **蓝色/****绿色****部署****传统上，蓝色/绿色部署意味着所有用户都切换到蓝色或绿色版本，一个是新版本，另一个是旧版本。如果切换到新版本时出现问题，则切换回旧版本非常简单。使用 Istio，可以通过逐渐将用户转移到新版本来改进此策略，例如，从 20%的用户开始，然后慢慢增加被路由到新版本的用户的百分比。在任何时候，如果在新版本中发现致命错误，将所有用户路由回旧版本都非常容易。**

 **正如[第 16 章](16.html)*中所述，*将我们的微服务部署到 Kubernetes*（请参阅*执行滚动升级*部分），重要的是要记住，这些类型的升级策略的先决条件是升级是向后兼容的。这种升级在用于与其他服务和数据库结构通信的 API 和消息格式方面都是兼容的。如果新版本的 microservice 需要更改旧版本无法处理的外部 API、消息格式或数据库结构，则无法应用这些升级策略。*

我们将在以下小节中介绍以下部署场景：

*   我们将首先部署`v1`和`v2`版本的微服务，路由配置为将所有请求发送到`v1`版本的微服务。
*   接下来，我们将允许一个测试组运行金丝雀测试；也就是说，我们将验证微服务的新`v2`版本。为了简化测试，我们只将测试用户路由到核心微服务的新版本，即`product`、`recommendation`和`review`微服务。
*   最后，我们将开始使用蓝色/绿色部署将普通用户转移到新版本：最初，一小部分用户，然后随着时间的推移，越来越多的用户，直到最终，他们都被路由到新版本。如果在新的 v2 版本中检测到致命错误，我们还将看到如何快速切换回`v1`版本。

首先让我们看看，为了部署微服务的两个并发版本`v1`和`v2`，对源代码进行了哪些更改。

# 源代码更改

如*更改 Kubernetes 定义文件的文件结构*一节所述，`kubernetes/services`中 Kubernetes 定义文件的文件结构在本章中已经扩展，以支持在生产环境中部署微服务的并发版本。展开的文件结构如下所示：

![](img/26f33b74-c0c1-46ea-80f3-259b536c3bf8.png)

有关开发环境的详细信息已从上图中删除。

首先让我们看看如何配置和创建`v1`和`v2`版本的微服务的服务和部署对象。之后，我们将查看用于控制路由的 Istio 对象的其他定义文件。

# 微服务并发版本的服务和部署对象

为了能够同时运行多个版本的微服务，部署对象及其对应的 POD 必须具有不同的名称，例如，`product-v1`和`product-v2`。然而，每个微服务必须只有一个 Kubernetes 服务对象。到特定微服务的所有流量始终通过同一个服务对象，而不管请求最终将路由到 pod 的哪个版本。这是通过使用 Kustomize 将部署对象和服务对象拆分到不同的文件夹中来实现的。

为了给部署对象及其 pods 版本相关的名称，`kustomization.yml`文件可以使用`nameSuffix`指令告诉 Kustomize 向它创建的所有 Kubernetes 对象添加给定的后缀。例如，`kubernetes/services/overlays/prod/v1`文件夹中用于`v1`版本微服务的`kustomization.yml`文件如下：

```
nameSuffix: -v1
bases:
- ../../../base/deployments
patchesStrategicMerge:
- ...
```

`nameSuffix: -v1`设置将导致使用此`kustomization.yml`文件创建的所有对象都以`-v1`后缀命名。

要创建没有版本后缀的对象，以及具有`v1`和`v2`版本后缀的部署对象及其 POD，`kubernetes/scripts/deploy-prod-env.bash`部署脚本执行单独的`kubectl apply`命令，如下所示：

```
kubectl apply -k kubernetes/services/base/services
kubectl apply -k kubernetes/services/overlays/prod/v1
kubectl apply -k kubernetes/services/overlays/prod/v2
```

我们还来看看为配置路由规则添加了哪些 Istio 定义文件。

# 为 Istio 添加了 Kubernetes 定义文件

要配置路由规则，我们将向`kubernetes/services/overlays/prod/istio`文件夹添加 Istio 对象。每个微服务都有一个虚拟服务对象，用于定义新旧版本之间路由的权重分布。最初，它被设置为将 100%的流量路由到旧版本。例如，`product-routing-virtual-service.yml`中产品微服务的路由规则如下：

```
  http:
  - route:
    - destination:
        host: product
        subset: old
      weight: 100
    - destination:
        host: product
        subset: new
      weight: 0
```

虚拟服务定义旧版本和新版本的子集。为了定义新旧版本的实际版本，每个微服务还定义了一个目标规则。目标规则详细说明了如何识别旧子集和新子集，例如，在`old_new_subsets_destination_rules.yml`中的产品微服务的情况下：

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: product-dr
spec:
  host: product
  subsets:
  - name: old
    labels:
      version: v1
  - name: new
    labels:
      version: v2
```

名为`old`的子集指向`version`标签设置为`v1`的产品吊舱，名为`new`的子集指向`version`标签设置为`v2`的吊舱

为了将流量路由到特定版本，Istio 文档建议使用名为`version`的标签标记 POD，以识别其版本。参见[https://istio.io/docs/setup/kubernetes/additional-setup/requirements/](https://istio.io/docs/setup/kubernetes/additional-setup/requirements/) 了解详情。

最后，为了支持金丝雀测试人员，虚拟服务中添加了一个额外的路由规则，用于三个核心微服务：产品、推荐和评审。此路由规则规定，将名为`X-group`的 HTTP 头设置为值`test`的任何传入请求将始终路由到新版本的服务。其内容如下：

```
  http:
  - match:
    - headers:
        X-group:
          exact: test
    route:
    - destination:
        host: product
        subset: new
```

`match`和`route`部分规定，将 HTTP 头`X-group`设置为值`test`的请求路由到名为`new`的子集。

为了创建这些 Istio 对象，`kubernetes/scripts/deploy-prod-env.bash`部署脚本执行以下命令：

```
kubectl apply -k kubernetes/services/overlays/prod/istio
```

最后，为了能够根据基于报头的路由将金丝雀测试员路由到新版本，`product-composite`微服务已更新为转发 HTTP 报头`X-group`。详见`se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl`类中的`getCompositeProduct()`方法。

现在，我们已经看到了对源代码的所有更改，我们已经准备好部署微服务的 v1 和 v2 版本。

# 部署 v1 和 v2 版本的微服务，并路由到 v1 版本

为了能够测试微服务的`v1`和`v2`版本，我们需要删除本章前面使用的开发环境，并创建一个生产环境，在其中我们可以部署微服务的 v1 和 v2 版本。

要实现此目的，请运行以下命令：

1.  重新创建`hands-on`命名空间：

```
kubectl delete namespace hands-on
kubectl create namespace hands-on
```

2.  通过使用以下命令运行脚本来执行部署：

```
./kubernetes/scripts/deploy-prod-env.bash 
```

该命令需要几分钟的时间，最终应列出 POD 的所有 v1 或 v2 版本，如下所示：

![](img/3ac160f5-b56c-46fd-8d26-d0cad647e4d5.png)

3.  运行常规测试以验证一切正常：

```
SKIP_CB_TESTS=true ./test-em-all.bash
```

如果在`deploy`命令之后立即执行此命令，则有时会失败。只需重新运行该命令，它就会正常运行！ 

由于我们现在为每个微服务运行两个 POD（版本 V1 和 V2），断路器测试不再工作。原因是测试脚本无法通过 Kubernetes 服务控制与哪个 pod 通信。测试脚本使用端口`4004`上的执行器端点询问`product-composite`微服务中断路器的状态。此端口不由 Istio 管理，因此其路由规则不适用。因此，测试脚本将不知道它是否正在检查`product-composite`微服务 V1 或 V2 中断路器的状态。我们可以使用`SKIP_CB_TESTS=true`标志跳过断路器测试。

预期输出与我们在前面章节中看到的类似，但不包括断路器测试：

![](img/4ebb9d21-ccfc-4210-92a0-8be37b37fd74.png)

我们现在准备运行一些*零停机部署*测试。让我们首先验证所有流量是否都流向 v1 版本的微服务！

# 验证所有流量最初是否都流向 v1 版本的微服务

为了验证所有请求是否路由到 v1 版本的微服务，我们将启动负载测试工具`siege`，然后使用 Kiali 观察流经服务网格的流量。

执行以下步骤：

1.  获取新的访问令牌并启动`siege`负载测试工具，使用以下命令：

```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)

siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1
```

2.  转到 Kiali 的 web UI 中的图形视图（[http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali) ：
    1.  单击显示菜单按钮并取消选择服务节点。
    2.  一两分钟后，只希望 v1 版本的微服务有如下流量：

![](img/e2170279-e949-462e-b545-b0b5c6c69d54.png)

好的这意味着，即使部署了 v2 版本的微服务，它们也不会获得任何路由到它们的流量。现在，让我们来尝试金丝雀测试，其中选定的测试用户可以尝试 v2 版本的微服务！

# 运行金丝雀测试

要运行金丝雀测试，换句话说，为了在所有其他用户仍被路由到已部署微服务的旧版本时路由到新版本，我们需要将`X-group`HTTP 头集添加到发送到外部 API 的请求中的值`test`。

要查看哪个版本的微服务提供了请求，可以检查响应中的`serviceAddresses`字段。`serviceAddresses`字段包含参与创建响应的每个服务的主机名。主机名等于 pod 的名称，因此我们可以在主机名中找到版本；例如，`product-v1-...`用于 V1 版本的产品服务，`product-v2-...`用于 V2 版本的产品服务。

让我们首先发送一个普通请求，并验证响应我们请求的是微服务的 v1 版本。接下来，发送一个将`X-group`HTTP 头设置为值`test`的请求，并验证新的 v2 版本是否有响应。

为此，请执行以下步骤：

1.  通过使用`jq`过滤掉响应中的`serviceAddresses`字段，执行正常请求以验证该请求是否路由到微服务的 v1 版本：

```
curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .serviceAddresses
```

期望得到以下方面的响应：

![](img/661a5a65-6610-40c6-aaf4-5a55f2e57b81.png)

正如所料，所有三个核心服务都是微服务的 v1 版本。

2.  如果我们添加`X-group=test`头，我们希望请求由核心微服务的 v2 版本提供服务。运行以下命令：

```
curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -H "X-group: test" | jq .serviceAddresses
```

预期会出现类似以下情况的响应：

![](img/1f63113a-ae33-446b-bdde-05fa4cf621c6.png)

正如预期的那样，所有三个响应的核心微服务现在都是 v2 版本；也就是说，作为一个金丝雀测试人员，我们被安排到新的 v2 版本！

鉴于金丝雀测试返回了预期的结果，我们准备允许普通用户使用蓝/绿部署路由到新的 v2 版本。

# 运行蓝色/绿色测试

为了将部分普通用户路由到新的 v2 版本的微服务，我们必须修改虚拟服务中的权重分布。目前为 100/0；换句话说，所有流量都路由到旧的 v1 版本。我们可以像以前一样实现这一点，即编辑`kubernetes/services/overlays/prod/istio`文件夹中虚拟服务的定义文件，然后运行`kubectl apply`命令使更改生效。或者，我们可以使用`kubectl patch`命令直接更改 Kubernetes API 服务器中虚拟服务对象的权重分布

我发现 patch 命令在对相同对象进行多次更改以尝试某些操作时非常有用，例如，更改路由规则中的权重分布。在本节中，我们将使用`kubectl patch`用于在 v1 和 v2 版本的微服务之间快速更改路由规则中的权重分布的命令。要在执行多个`kubectl patch`命令后获取虚拟服务的状态，可以发出`kubectl get vs NNN -o yaml`等命令。例如，要获取产品微服务的虚拟服务的状态，请发出以下命令：`kubectl get vs product-vs -o yaml`。

因为我们以前没有使用过`kubectl patch`命令，而且它可能会有点复杂，所以在执行绿色/蓝色部署之前，让我们简要介绍一下它是如何工作的。

# kubectl patch 命令简介

`kubectl patch`命令可用于更新 Kubernetes API 服务器中现有对象中的特定字段。我们将在 review microservice 的虚拟服务上尝试 patch 命令，名为`review-vs`。虚拟服务定义的相关部分`review-vs`如下所示：

```
spec:
  http:
  - match:
    ...
  - route:
    - destination:
        host: review
        subset: old
      weight: 100
    - destination:
        host: review
        subset: new
      weight: 0 
```

完整的源代码请参考`kubernetes/services/overlays/prod/istio/review-routing-virtual-service.yml`。

将路由的权重分布更改为`review`微服务中 v1 和 v2 POD 的样例补丁命令如下所示：

```
kubectl patch virtualservice review-vs --type=json -p='[
 {"op": "add", "path": "/spec/http/1/route/0/weight", "value": "80"},
 {"op": "add", "path": "/spec/http/1/route/1/weight", "value": "20"}
]'
```

该命令将配置 review microservice 的路由规则，将 80%的请求路由到旧版本，将 20%的请求路由到新版本。

为指定在`review-vs`虚拟服务中更改`weight`值，旧版本提供`/spec/http/1/route/0/weight`路径，新版本提供`/spec/http/1/route/1/weight`路径

路径中的`0`和`1`用于指定虚拟服务定义中数组元素的索引。例如，`http/1`表示数组中`http`元素下的第二个元素。参见前面`review-vs`虚拟服务的定义。

从前面的定义可以看出，第二个元素是`route`元素。索引为`0`的第一个元素为匹配元素。

现在我们对`kubectl patch`命令有了更多的了解，我们准备测试一个蓝/绿部署。

# 执行蓝色/绿色部署

现在，是时候逐步将越来越多的用户转移到使用蓝/绿部署的新版本了。要执行部署，请运行以下步骤：

1.  确保负载测试工具`Siege`仍在运行。
    在前面的*中启动，验证所有流量最初都流向微服务*部分的 v1 版本。
2.  要允许 20%的用户路由到 review microservice 的新 v2 版本，我们可以使用以下命令修补虚拟服务并更改权重：

```
kubectl patch virtualservice review-vs --type=json -p='[
 {"op": "add", "path": "/spec/http/1/route/0/weight", "value": 
  "80"},
 {"op": "add", "path": "/spec/http/1/route/1/weight", "value":  
  "20"}
 ]'
```

3.  要观察路由规则的变化，请转到 Kiali web UI（[http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali) ）并选择图形视图。
4.  将边缘标签更改为`Requests percentage`。
5.  请等待一分钟，然后在 Kiali 中更新静力学，以便我们可以观察到变化。预计 Kiali 中的图形将显示以下内容：

![](img/a610a4da-ad35-4bbd-ab61-f30f72241355.png)

根据您等待的时间，图表可能会有所不同！
在屏幕截图中，我们可以看到 Istio 现在将流量路由到`review`微服务的 v1 和 v2 版本。

在从`product-composite`微服务发送到`review`微服务的 33%流量中，7%路由到新的 v2 pod，26%路由到旧的 v1 pod。这意味着 7/33（=21%）的请求被路由到 v2 pod，26/33（=79%）路由到 v1 pod。这与我们要求的 20/80 发行版一致：

1.  请随意尝试前面的`kubectl patch`命令，以影响其他核心微服务的路由规则：`product`和`recommendation`。
2.  如果要将所有流量路由到所有 MicroService 的 v2 版本，可以运行以下脚本：

```
./kubernetes/scripts/route-all-traffic-to-v2-services.bash
```

在 Kiali 能够可视化 v1 和 v2 版本的微服务之间的路由变化之前，您必须给 Kiali 一两分钟时间来收集指标，但请记住，实际路由的变化是即时的！

一段时间后，图中只显示 v2 版本的微服务：

![](img/fafbc4a4-4e90-47d5-8a5f-b933a548edf3.png)

根据您等待的时间，图表可能会有所不同！

3.  如果升级到 v2 后出现严重问题，可以执行以下命令将所有流量恢复到所有微服务的 v1 版本：

```
./kubernetes/scripts/route-all-traffic-to-v1-services.bash
```

过了一会儿，Kiali 中的图形应该看起来像上一个*中的屏幕截图，验证所有流量最初都流向微服务*部分的 v1 版本；也就是说，设想所有请求再次转到所有微服务的 v1 版本。

本文结束了对服务网格概念的介绍，并将 Istio 作为该概念的实现。

在结束本章之前，让我们回顾一下如何在 Docker Compose 中运行测试，以确保微服务的源代码不依赖于 Kubernetes 中的部署。

# 使用 Docker Compose 运行测试

如[第 17 章](17.html)所述，*实现 Kubernetes 功能作为替代*（参考*验证微服务在没有 Kubernetes*部分的情况下工作），从功能角度来看，确保微服务的源代码不依赖于 Kubernetes 或 Istio 等平台是很重要的。

为了验证微服务在没有 Kubernetes 和 Istio 的情况下能够按预期工作，请按照[第 1 章*7*](17.html)*中所述运行测试，实现 Kubernetes 功能作为替代*（请参阅使用 Docker Compose 部分进行测试）。由于测试脚本`test-em-all.bash`的默认值已更改，如前面在*运行创建服务网格的命令*部分中所述，因此在使用 Docker Compose 时必须设置以下参数：`HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443`。例如，要使用默认 Docker Compose 文件`docker-compose.yml`运行测试，请运行以下命令：

```
HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443 ./test-em-all.bash start stop
```

如前所述，试验应从启动所有容器开始；然后它应该运行测试，并最终停止所有容器。有关预期输出的详细信息，请参见[第 17 章](17.html)、*实现 Kubernetes 特性作为替代*（请参阅*验证微服务在没有 Kubernetes 的情况下工作*部分）

在使用 Docker Compose 成功执行测试之后，我们已经从功能角度验证了微服务既不依赖于 Kubernetes，也不依赖于 Istio。这些测试结束了将 Istio 用作服务网格的一章

# 总结

在本章中，我们学习了服务网格概念和 Istio，一个实现它的开源项目。服务网格提供了在安全、策略实施、恢复能力和流量管理等领域的微服务系统环境中应对挑战的能力。服务网格还可以通过可视化流经微服务的流量，使微服务的系统景观可见。

为了便于观察，Istio 使用了 Kiali、Jaeger 和 Grafana（有关 Grafana 的更多信息，请参见[第 20 章](20.html)、*监控微服务*），可以将 Istio 配置为使用证书保护带有 HTTPS 的外部 API，并要求外部请求包含有效的基于 JWT 的 OAuth 2.0/OIDC 访问令牌。最后，Istio 可以配置为使用**相互认证**（**mTLS**自动保护内部通信。

对于弹性和鲁棒性，Istio 提供了处理重试、超时的机制，以及类似于断路器的异常检测机制。在许多情况下，如果可能的话，最好在微服务的源代码中实现这些恢复能力。Istio 中注入故障和延迟的能力对于验证服务网格中的微服务是否作为一个弹性和健壮的系统环境一起工作非常有用。Istio 还可用于处理零停机部署。使用它的细粒度路由规则，可以执行 canary 和 blue/green 部署。

我们尚未涉及的一个重要领域是如何收集和分析所有 microservice 实例创建的日志文件。在下一章中，我们将看到如何使用基于 Elasticsearch、Fluentd 和 Kibana 的流行工具堆栈（称为 EFK 堆栈）来实现这一点。

# 问题

1.  服务网格中代理组件的用途是什么？
2.  服务网格中的控制平面和数据平面有什么区别？
3.  `istioctl kube-inject`命令的作用是什么？
4.  `minikube tunnel`命令的作用是什么？
5.  Istio 中使用了哪些工具用于观察？
6.  使用相互身份验证使 Istio 保护服务网格内的通信需要什么配置？
7.  虚拟服务中的`abort`和`delay`元素可以用来做什么？
8.  设置蓝色/绿色部署场景需要什么配置？*********